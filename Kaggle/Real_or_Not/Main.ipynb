{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input, concatenate , LSTM\n",
    "from keras import Model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix , f1_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from openpyxl import load_workbook\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_proba_threshold(y_positive_proba_pred, threshold):\n",
    "    apply_threshold = np.vectorize(lambda x:0 if x< threshold else 1)\n",
    "    return apply_threshold(y_positive_proba_pred)\n",
    "def optimize_binary_threshold(y_proba_pred, y_true, thresholds, metric = 'f1'):\n",
    "    best_score = 0\n",
    "    best_thresh = 0.1\n",
    "    \n",
    "    for thresh in thresholds : \n",
    "        y_pred = apply_proba_threshold(y_proba_pred, thresh)\n",
    "        \n",
    "        score = f1_score(y_pred = y_pred,y_true = y_true)\n",
    "        \n",
    "            \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thresh = thresh\n",
    "    \n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding chargé\n"
     ]
    }
   ],
   "source": [
    "### embedding ###\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "print('embedding chargé')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lecture des données ####\n",
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "test = pd.read_csv(\"Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = train[['text','keyword','location']]\n",
    "ytrain = train['target']\n",
    "xtest = test[['text','keyword','location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_at(text):\n",
    "    p = re.compile(r'\\s')\n",
    "    l = p.split(text)\n",
    "    for mot in l :\n",
    "        if '@' in mot :\n",
    "            l.remove(mot)\n",
    "    text_final = ''\n",
    "    for mot in l:\n",
    "        text_final +=mot\n",
    "        text_final += ' '\n",
    "    return text_final\n",
    "def text_traitement(text_init):\n",
    "    text_init = no_at(text_init)\n",
    "    text_final = ''\n",
    "    for carac in text_init:\n",
    "        print(\"no\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-a4ef583013c5>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain['text'] = xtrain['text'].apply(no_at)\n",
      "<ipython-input-21-a4ef583013c5>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest['text'] = xtest['text'].apply(no_at)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    xtrain['text'] = xtrain['text'].apply(no_at)\n",
    "    xtest['text'] = xtest['text'].apply(no_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text keyword location\n",
      "0     Our Deeds are the Reason of this #earthquake M...     NaN      NaN\n",
      "1              Forest fire near La Ronge Sask. Canada       NaN      NaN\n",
      "2     All residents asked to 'shelter in place' are ...     NaN      NaN\n",
      "3     13,000 people receive #wildfires evacuation or...     NaN      NaN\n",
      "4     Just got sent this photo from Ruby #Alaska as ...     NaN      NaN\n",
      "...                                                 ...     ...      ...\n",
      "7608  Two giant cranes holding a bridge collapse int...     NaN      NaN\n",
      "7609  The out of control wild fires in California ev...     NaN      NaN\n",
      "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...     NaN      NaN\n",
      "7611  Police investigating after an e-bike collided ...     NaN      NaN\n",
      "7612  The Latest: More Homes Razed by Northern Calif...     NaN      NaN\n",
      "\n",
      "[7613 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keyword = {}\n",
    "j=0\n",
    "for i in set(train['keyword']):\n",
    "    dict_keyword[i] = j\n",
    "    j+=1\n",
    "dict_location = {}\n",
    "j=0\n",
    "for i in set(train['location']):\n",
    "    dict_location[i] = j\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes :  ['id', 'keyword', 'location', 'text', 'target']\n",
      "Nombre de lignes du set de train :  7613\n",
      "Nombre de lignes du set de test :  3263\n",
      "Les classes sont :  {0, 1}\n"
     ]
    }
   ],
   "source": [
    "### caractéristiques des données ###\n",
    "print(\"Colonnes : \",list(train.columns))\n",
    "print(\"Nombre de lignes du set de train : \", len(train))\n",
    "print(\"Nombre de lignes du set de test : \", len(test))\n",
    "print(\"Les classes sont : \",set(train['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x000002733C7BE670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x000002733C7BE670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "### Split train into train and validation ###\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(xtrain, ytrain, test_size = 0.3)\n",
    "xtrain_text = np.array(embed(xtrain['text']))\n",
    "xtrain_location = np.array([dict_location[i] for i in xtrain['location']])\n",
    "xtrain_keyword = np.array([dict_keyword[i] for i in xtrain['keyword']])\n",
    "\n",
    "xvalid_text = np.array(embed(xvalid['text']))\n",
    "xvalid_location = np.array([dict_location[i] for i in xvalid['location']])\n",
    "xvalid_keyword = np.array([dict_keyword[i] for i in xvalid['keyword']]) \n",
    "\n",
    "xtest_text = np.array(embed(xtest['text']))\n",
    "xtest_location = []\n",
    "xtest_keyword = []\n",
    "\n",
    "for i in xtest['location']:\n",
    "    if i in dict_location:\n",
    "        xtest_location.append(dict_location[i])\n",
    "    else:\n",
    "        xtest_location.append(0)\n",
    "\n",
    "for i in xtest['keyword']:\n",
    "    if i in dict_keyword:\n",
    "        xtest_keyword.append(dict_keyword[i])\n",
    "    else:\n",
    "        xtest_keyword.append(0)\n",
    "        \n",
    "xtest_location = np.array(xtest_location)\n",
    "xtest_keyword = np.array(xtest_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_xgb = []\n",
    "for i in range(len(xtrain_text)):\n",
    "    m = list(xtrain_text[i])\n",
    "    m.append(xtrain_location[i])\n",
    "    m.append(xtrain_keyword[i])\n",
    "    xtrain_xgb.append(m)\n",
    "xtrain_xgb=np.array(xtrain_xgb)\n",
    "\n",
    "xvalid_xgb = []\n",
    "for i in range(len(xvalid_text)):\n",
    "    m = list(xvalid_text[i])\n",
    "    m.append(xvalid_location[i])\n",
    "    m.append(xvalid_keyword[i])\n",
    "    xvalid_xgb.append(m)\n",
    "xvalid_xgb=np.array(xvalid_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1214  103]\n",
      " [ 307  660]]\n",
      "0.7630057803468208\n"
     ]
    }
   ],
   "source": [
    "# random forest \n",
    "clf = RandomForestClassifier(n_estimators = 350,max_depth=20, random_state=70)\n",
    "clf.fit(xtrain_xgb, ytrain)\n",
    "y_pred = clf.predict(xvalid_xgb)\n",
    "CM = confusion_matrix(y_pred = y_pred,y_true = yvalid)\n",
    "f1 = f1_score(y_pred = y_pred,y_true = yvalid)\n",
    "print(CM)\n",
    "print(f1)\n",
    "# print(clf.predict_proba(xvalid_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0.05, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.15, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=400, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### xgboost model learning####\n",
    "model = XGBClassifier(learning_rate = 0.15, n_estimators=400, max_depth=10,gamma=0.05)\n",
    "model.fit(xtrain_xgb, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1182  135]\n",
      " [ 273  694]]\n",
      "0.7728285077951002\n"
     ]
    }
   ],
   "source": [
    "### xgboost model predict####\n",
    "y_pred = model.predict(xvalid_xgb)\n",
    "CM = confusion_matrix(y_pred = y_pred,y_true = yvalid)\n",
    "f1 = f1_score(y_pred = y_pred,y_true = yvalid)\n",
    "print(CM)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=20)\n",
    "cb_list=[es]\n",
    "def b():\n",
    "    inputs_text = Input(shape=(512,))\n",
    "    \n",
    "    inputs_keyword = Input(shape=(1,))\n",
    "    \n",
    "    #inputs_location = Input(shape=(1,))\n",
    "\n",
    "    \n",
    "    \n",
    "    first_layer_text = Dense(60, activation='sigmoid')(inputs_text)\n",
    "    second_layer_text = Dense(30, activation='softplus')(first_layer_text)\n",
    "    \n",
    "    first_layer_keyword = Dense(1, activation='sigmoid')(inputs_keyword)\n",
    "    second_layer_keyword = Dense(1, activation='softsign')(first_layer_keyword)\n",
    "    \n",
    "    #first_layer_location = Dense(1, activation='sigmoid')(inputs_location)\n",
    "    #second_layer_location = Dense(1, activation='sigmoid')(first_layer_location)\n",
    "    \n",
    "    \n",
    "    pre_out = concatenate([second_layer_text,second_layer_keyword])\n",
    "    \n",
    "    pre_out_out = Dense(30, activation='sigmoid')(pre_out)\n",
    "    \n",
    "    out = Dense(1,activation='sigmoid')(pre_out_out)\n",
    "    \n",
    "    loss_f = 'binary_crossentropy'\n",
    "    \n",
    "    model = Model(inputs=[inputs_text,inputs_keyword], outputs=[out])\n",
    "    \n",
    "    model.compile(optimizer='adam',loss=[loss_f],metrics=['accuracy'])\n",
    "    \n",
    "    model.fit([xtrain_text,xtrain_keyword],ytrain, epochs=200,batch_size=128,callbacks=cb_list,validation_split=0.1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.7479 - accuracy: 0.4912 - val_loss: 0.6961 - val_accuracy: 0.5478\n",
      "Epoch 2/200\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.6707 - accuracy: 0.5811 - val_loss: 0.6745 - val_accuracy: 0.5478\n",
      "Epoch 3/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6548 - accuracy: 0.5811 - val_loss: 0.6542 - val_accuracy: 0.5535\n",
      "Epoch 4/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6235 - accuracy: 0.6293 - val_loss: 0.6103 - val_accuracy: 0.7242\n",
      "Epoch 5/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.5561 - accuracy: 0.7923 - val_loss: 0.5326 - val_accuracy: 0.7617\n",
      "Epoch 6/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.8036 - val_loss: 0.4700 - val_accuracy: 0.7861\n",
      "Epoch 7/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8130 - val_loss: 0.4512 - val_accuracy: 0.7824\n",
      "Epoch 8/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4149 - accuracy: 0.8196 - val_loss: 0.4427 - val_accuracy: 0.7842\n",
      "Epoch 9/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.8228 - val_loss: 0.4405 - val_accuracy: 0.7861\n",
      "Epoch 10/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4003 - accuracy: 0.8234 - val_loss: 0.4447 - val_accuracy: 0.7861\n",
      "Epoch 11/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.8299 - val_loss: 0.4402 - val_accuracy: 0.7974\n",
      "Epoch 12/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3927 - accuracy: 0.8296 - val_loss: 0.4341 - val_accuracy: 0.7936\n",
      "Epoch 13/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3890 - accuracy: 0.8351 - val_loss: 0.4389 - val_accuracy: 0.7992\n",
      "Epoch 14/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3865 - accuracy: 0.8328 - val_loss: 0.4382 - val_accuracy: 0.8011\n",
      "Epoch 15/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3848 - accuracy: 0.8355 - val_loss: 0.4392 - val_accuracy: 0.7974\n",
      "Epoch 16/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3839 - accuracy: 0.8355 - val_loss: 0.4372 - val_accuracy: 0.7992\n",
      "Epoch 17/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3822 - accuracy: 0.8372 - val_loss: 0.4405 - val_accuracy: 0.7936\n",
      "Epoch 18/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3799 - accuracy: 0.8367 - val_loss: 0.4360 - val_accuracy: 0.7992\n",
      "Epoch 19/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.8390 - val_loss: 0.4407 - val_accuracy: 0.7936\n",
      "Epoch 20/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.8401 - val_loss: 0.4330 - val_accuracy: 0.7974\n",
      "Epoch 21/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3779 - accuracy: 0.8390 - val_loss: 0.4339 - val_accuracy: 0.7936\n",
      "Epoch 22/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3767 - accuracy: 0.8405 - val_loss: 0.4352 - val_accuracy: 0.7899\n",
      "Epoch 23/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3756 - accuracy: 0.8411 - val_loss: 0.4426 - val_accuracy: 0.7936\n",
      "Epoch 24/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3740 - accuracy: 0.8415 - val_loss: 0.4351 - val_accuracy: 0.7936\n",
      "Epoch 25/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8411 - val_loss: 0.4427 - val_accuracy: 0.7899\n",
      "Epoch 26/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3727 - accuracy: 0.8411 - val_loss: 0.4385 - val_accuracy: 0.7880\n",
      "Epoch 27/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3723 - accuracy: 0.8451 - val_loss: 0.4430 - val_accuracy: 0.7899\n",
      "Epoch 28/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8445 - val_loss: 0.4411 - val_accuracy: 0.7824\n",
      "Epoch 29/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3704 - accuracy: 0.8442 - val_loss: 0.4368 - val_accuracy: 0.7880\n",
      "Epoch 30/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.8424 - val_loss: 0.4462 - val_accuracy: 0.7917\n",
      "Epoch 31/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3694 - accuracy: 0.8465 - val_loss: 0.4360 - val_accuracy: 0.7899\n",
      "Epoch 32/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8453 - val_loss: 0.4425 - val_accuracy: 0.7917\n",
      "Epoch 33/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3694 - accuracy: 0.8474 - val_loss: 0.4473 - val_accuracy: 0.7936\n",
      "Epoch 34/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8438 - val_loss: 0.4532 - val_accuracy: 0.7992\n",
      "Epoch 35/200\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3669 - accuracy: 0.8451 - val_loss: 0.4479 - val_accuracy: 0.7936\n",
      "Epoch 36/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3666 - accuracy: 0.8463 - val_loss: 0.4393 - val_accuracy: 0.7880\n",
      "Epoch 37/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8455 - val_loss: 0.4482 - val_accuracy: 0.7917\n",
      "Epoch 38/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3664 - accuracy: 0.8447 - val_loss: 0.4376 - val_accuracy: 0.7917\n",
      "Epoch 39/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3645 - accuracy: 0.8490 - val_loss: 0.4393 - val_accuracy: 0.7899\n",
      "Epoch 40/200\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3648 - accuracy: 0.8467 - val_loss: 0.4469 - val_accuracy: 0.7880\n",
      "Epoch 00040: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = b()\n",
    "ypred = model.predict([xvalid_text,xvalid_keyword])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999\n",
      "0.359\n",
      "[[1079  184]\n",
      " [ 234  787]]\n",
      "0.7901606425702811\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.1+i*0.001 for i in range(900)]\n",
    "print(max(thresholds))\n",
    "thresh = optimize_binary_threshold(ypred, yvalid, thresholds, metric = 'f1')\n",
    "print(thresh)\n",
    "y_pred = apply_proba_threshold(ypred, thresh)\n",
    "CM = confusion_matrix(y_pred = y_pred,y_true = yvalid)\n",
    "f1 = f1_score(y_pred = y_pred,y_true = yvalid)\n",
    "print(CM)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "idd = test['id']\n",
    "predictions = model.predict([xtest_text,xtest_location,xtest_keyword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pred for submission ###\n",
    "idd = test['id']\n",
    "predict = model.predict([xtest_text,xtest_location,xtest_keyword])\n",
    "predictions = apply_proba_threshold(predict, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  target\n",
      "0         0       1\n",
      "1         2       1\n",
      "2         3       1\n",
      "3         9       1\n",
      "4        11       1\n",
      "...     ...     ...\n",
      "3258  10861       1\n",
      "3259  10865       1\n",
      "3260  10868       1\n",
      "3261  10874       1\n",
      "3262  10875       1\n",
      "\n",
      "[3263 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "### save data ####\n",
    "path = 'Submission/'\n",
    "file_name = 'SubmissionV0.csv'\n",
    "df = pd.DataFrame(idd)\n",
    "df['target'] = predictions\n",
    "print(df)\n",
    "df.to_csv(path+file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
